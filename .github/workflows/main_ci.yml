name: CI DST Airlines - Infrastructure

on:
  push:
    branches: [ main, master, refactoring ]
  pull_request:
    branches: [ main, master, refactoring ]

jobs:
  # JOB 1 : Tests API (Python + Postgres avec données réelles)
  api:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: user_test
          POSTGRES_PASSWORD: password_test
          POSTGRES_DB: db_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest httpx pandas numpy
          if [ -f api/requirements.txt ]; then pip install -r api/requirements.txt; fi

      - name: Setup Test DB Tables & Data
        env:
          PGPASSWORD: password_test
        run: |
          # 1. On crée la base 'user_test' (celle que ton API cherche par défaut)
          psql -h localhost -U user_test -d postgres -c "CREATE DATABASE user_test;"

          # 2. On boucle pour créer la structure et injecter les données dans les DEUX bases
          # Cela garantit que peu importe où ton code regarde, il trouve les données.
          for DB in db_test user_test; do
            echo "Configuring database: $DB"
            psql -h localhost -U user_test -d $DB -c "
            CREATE TABLE IF NOT EXISTS flight_static (
                callsign VARCHAR(10) PRIMARY KEY,
                airline_name VARCHAR(100),
                origin_code VARCHAR(3),
                destination_code VARCHAR(3),
                commercial_flight BOOLEAN
            );

            CREATE TABLE IF NOT EXISTS flight_dynamic (
                callsign VARCHAR(10) NOT NULL,
                icao24 VARCHAR(10) NOT NULL,
                flight_date DATE NOT NULL,
                departure_scheduled TIME NOT NULL,
                departure_actual TIME,
                arrival_scheduled TIME,
                arrival_actual TIME,
                status VARCHAR(15) NOT NULL,
                last_update TIMESTAMPTZ DEFAULT NOW(),
                unique_key TEXT NOT NULL,
                CONSTRAINT pk_flight_dynamic_$DB PRIMARY KEY (unique_key)
            );

            CREATE TABLE IF NOT EXISTS live_data (
                indice SERIAL,
                request_id UUID NOT NULL,
                callsign VARCHAR(10) NOT NULL,
                icao24 VARCHAR(10) NOT NULL,
                flight_date DATE NOT NULL,
                departure_scheduled TIME NOT NULL,
                longitude DOUBLE PRECISION,
                latitude DOUBLE PRECISION,
                baro_altitude DOUBLE PRECISION,
                geo_altitude DOUBLE PRECISION, 
                on_ground BOOLEAN,
                velocity DOUBLE PRECISION,
                vertical_rate DOUBLE PRECISION,
                temperature DOUBLE PRECISION,
                wind_speed DOUBLE PRECISION,
                gust_speed DOUBLE PRECISION,
                visibility DOUBLE PRECISION,
                cloud_coverage DOUBLE PRECISION,
                rain DOUBLE PRECISION,
                global_condition VARCHAR(100),
                unique_key TEXT NOT NULL,
                CONSTRAINT pk_live_data_$DB PRIMARY KEY (request_id, unique_key),
                CONSTRAINT fk_live_dynamic_$DB FOREIGN KEY(unique_key) 
                    REFERENCES flight_dynamic(unique_key)
            );
            "
            # Injection du seed dans la base courante
            psql -h localhost -U user_test -d $DB -f api/tests/seed_data.sql
          done

      - name: Run API Unit Tests
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: user_test
          POSTGRES_PASSWORD: password_test
          POSTGRES_DB: db_test
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python -m pytest -v api/tests/test_api.py

  # JOB 2 : Validation Terraform
  terraform:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0
      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform
      - name: Terraform Validate
        run: terraform validate
        working-directory: ./terraform
      - name: Terraform Format Check
        run: terraform fmt -check
        working-directory: ./terraform

  # JOB 3 : Validation Airflow (DAGs + Plugins)
  airflow:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install Airflow & Deps
        run: |
          python -m pip install --upgrade pip
          pip install "apache-airflow>=2.10.0" \
                      psycopg2-binary \
                      apache-airflow-providers-postgres \
                      apache-airflow-providers-http \
                      apache-airflow-providers-common-sql \
                      prometheus-client pandas pyarrow mlflow scikit-learn requests selenium
      - name: Static DAG Load Test
        env:
          AIRFLOW_VAR_PUSHGATEWAY_URL: "localhost:9091"
          AIRFLOW_VAR_CONNECTION_ID: "airlines"
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow/plugins
          python -c "from airflow.models import DagBag; dagbag = DagBag(dag_folder='airflow/dags', include_examples=False); print('Import Errors Found:', dagbag.import_errors); exit(1 if len(dagbag.import_errors) > 0 else 0)"

  # JOB 4 : Vérification du Build Docker (Dry Run)
  docker:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Create external resources for CI
        run: |
          docker network create airflow_network_terraform
          docker volume create postgres-db-volume-terraform
          docker volume create mlflow-artifacts-terraform
          docker volume create airflow-data-terraform
          mkdir -p airflow/config
          echo "{}" > airflow/config/variables.json
          
      - name: Validate Docker Compose Config
        env:
          AIRFLOW_JWT_SECRET: "ci_static_secret_32_characters_long"
          AIRFLOW_FERNET_KEY: "ci_static_fernet_key_validation="
          AIRFLOW_API_SECRET_KEY: "ci_static_api_secret_key_32_char"
          POSTGRES_USER: "test"
          POSTGRES_PASSWORD: "test"
          POSTGRES_PORT: 5432
          AIRFLOW_POSTGRES_DB: "test"
          AIRLINES_POSTGRES_DB: "test"
          MLFLOW_API_URL: "http://localhost:5000"
          SELENIUM_POOL_SIZE: 2
          AIRFLOW_PROJ_DIR: "."
        run: docker compose config
        
      - name: Build Airflow Image (Dry Run)
        run: docker compose build airflow-init